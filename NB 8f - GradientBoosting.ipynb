{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to try gradient boosting on the ensembled models\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"  # Target variable\n",
    "X = merged_data.drop(columns=[target, \"material\"])  # Drop 'material' column\n",
    "y = merged_data[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Optimized LightGBM Model\n",
    "optimized_lgb = lgb.LGBMRegressor(n_estimators=496, max_depth=15, learning_rate=0.057878589503943714, \n",
    "                                  subsample=0.6619352139576826, colsample_bytree=0.7512301369524537, \n",
    "                                  num_leaves=148, verbose=-1, force_col_wise=True)\n",
    "optimized_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Train Optimized XGBoost Model\n",
    "optimized_xgb = xgb.XGBRegressor(n_estimators=407, max_depth=10, learning_rate=0.02962746174406205,\n",
    "                                 subsample=0.8786056663685927, colsample_bytree=0.6260167856358314,\n",
    "                                 gamma=4.321388407974591, tree_method=\"hist\", random_state=42)\n",
    "optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for meta-model training\n",
    "y_pred_lgb_train = optimized_lgb.predict(X_train)\n",
    "y_pred_xgb_train = optimized_xgb.predict(X_train)\n",
    "\n",
    "# Generate predictions for meta-model testing\n",
    "y_pred_lgb_test = optimized_lgb.predict(X_test)\n",
    "y_pred_xgb_test = optimized_xgb.predict(X_test)\n",
    "\n",
    "# Stack predictions as new features\n",
    "X_meta_train = np.column_stack((y_pred_lgb_train, y_pred_xgb_train))\n",
    "X_meta_test = np.column_stack((y_pred_lgb_test, y_pred_xgb_test))\n",
    "\n",
    "# Train Meta-Learner (Gradient Boosting)\n",
    "meta_model = GradientBoostingRegressor(n_estimators=200, max_depth=3, learning_rate=0.05, random_state=42)\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Meta Model Predictions\n",
    "y_pred_meta = meta_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluate Meta Model\n",
    "meta_rmse = np.sqrt(mean_squared_error(y_test, y_pred_meta))\n",
    "meta_r2 = r2_score(y_test, y_pred_meta)\n",
    "\n",
    "print(f\"Meta-Learning Model (Gradient Boosting) - Test RMSE: {meta_rmse:.4f}, Test R²: {meta_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rsults:\n",
    "\n",
    "Meta-Learning Model (Gradient Boosting) - Test RMSE: 8.5433, Test R²: 0.9366"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
