{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and test a CatBoost model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the data\n",
    "main_data = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "\n",
    "# 'critical_temp' is the target\n",
    "X = main_data.drop(\"critical_temp\", axis=1)\n",
    "y = main_data[\"critical_temp\"]\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a baseline CatBoost model\n",
    "\n",
    "\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=500,             # Number of boosting rounds (trees)\n",
    "    learning_rate=0.03,         # Step size for updating weights; lower values usually require more iterations but can lead to better generalization\n",
    "    depth=6,                    # Maximum depth of the trees; controls the model’s complexity\n",
    "    l2_leaf_reg=3,              # L2 regularization coefficient to reduce overfitting\n",
    "    loss_function='RMSE',       # Loss function to optimize, here RMSE for regression tasks\n",
    "    random_seed=42,             # Ensures reproducibility\n",
    "    verbose=100                 # Controls how often training progress is printed (set to 0 to disable)\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using RMSE and R² metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Base 80/20 CatBoost Performance:\")\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"R²: {:.4f}\".format(r2))\n",
    "\n",
    "# Optional Cross-validation to further assess model performance\n",
    "cv_scores = cross_val_score(catboost_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "cv_rmse = -np.mean(cv_scores)\n",
    "print(\"Cross-validated RMSE: {:.4f}\".format(cv_rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with training data, original features:\n",
    "\n",
    "400:\tlearn: 10.9660427\ttotal: 6.85s\tremaining: 1.69s\n",
    "499:\tlearn: 10.4761130\ttotal: 8.51s\tremaining: 0us\n",
    "\n",
    "Base 80/20 CatBoost Performance:\n",
    "\n",
    "RMSE: 10.9983\n",
    "\n",
    "R²: 0.8949\n",
    "\n",
    "Cross-validated RMSE: 11.4960\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#made a catboost with all features\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"\n",
    "X = merged_data.drop(columns=['critical_temp', 'material'], axis=1)\n",
    "y = merged_data[target]\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a baseline CatBoost model\n",
    "\n",
    "\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=500,             # Number of boosting rounds (trees)\n",
    "    learning_rate=0.03,         # Step size for updating weights; lower values usually require more iterations but can lead to better generalization\n",
    "    depth=6,                    # Maximum depth of the trees; controls the model’s complexity\n",
    "    l2_leaf_reg=3,              # L2 regularization coefficient to reduce overfitting\n",
    "    loss_function='RMSE',       # Loss function to optimize, here RMSE for regression tasks\n",
    "    random_seed=42,             # Ensures reproducibility\n",
    "    verbose=100                 # Controls how often training progress is printed (set to 0 to disable)\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using RMSE and R² metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Base 80/20 CatBoost Performance:\")\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"R²: {:.4f}\".format(r2))\n",
    "\n",
    "# Optional Cross-validation to further assess model performance\n",
    "cv_scores = cross_val_score(catboost_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "cv_rmse = -np.mean(cv_scores)\n",
    "print(\"Cross-validated RMSE: {:.4f}\".format(cv_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "\n",
    "400:\tlearn: 10.9568660\ttotal: 9.56s\tremaining: 2.36s\n",
    "499:\tlearn: 10.5263805\ttotal: 12.2s\tremaining: 0us\n",
    "\n",
    "Base 80/20 CatBoost Performance:\n",
    "\n",
    "RMSE: 10.8753\n",
    "\n",
    "R²: 0.8973\n",
    "\n",
    "Cross-validated RMSE: 11.4212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with all features plus 3 engineered\n",
    "\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Feature Engineering: Physics-Based Ratio, Thermal Conductivity Transformation, Log transformation\n",
    "merged_data[\"mass_density_ratio\"] = merged_data[\"wtd_mean_atomic_mass\"] / (merged_data[\"wtd_mean_Density\"] + 1e-9)\n",
    "merged_data[\"affinity_valence_ratio\"] = merged_data[\"wtd_mean_ElectronAffinity\"] / (merged_data[\"wtd_mean_Valence\"] + 1e-9)\n",
    "merged_data[\"log_thermal_conductivity\"] = np.log1p(merged_data[\"range_ThermalConductivity\"])\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"\n",
    "X = merged_data.drop(columns=['critical_temp', 'material'], axis=1)\n",
    "y = merged_data[target]\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a baseline CatBoost model\n",
    "\n",
    "\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=500,             # Number of boosting rounds (trees)\n",
    "    learning_rate=0.03,         # Step size for updating weights; lower values usually require more iterations but can lead to better generalization\n",
    "    depth=6,                    # Maximum depth of the trees; controls the model’s complexity\n",
    "    l2_leaf_reg=3,              # L2 regularization coefficient to reduce overfitting\n",
    "    loss_function='RMSE',       # Loss function to optimize, here RMSE for regression tasks\n",
    "    random_seed=42,             # Ensures reproducibility\n",
    "    verbose=100                 # Controls how often training progress is printed (set to 0 to disable)\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using RMSE and R² metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Base 80/20 CatBoost Performance:\")\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"R²: {:.4f}\".format(r2))\n",
    "\n",
    "# Optional Cross-validation to further assess model performance\n",
    "cv_scores = cross_val_score(catboost_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "cv_rmse = -np.mean(cv_scores)\n",
    "print(\"Cross-validated RMSE: {:.4f}\".format(cv_rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results all features (including engineered):\n",
    "\n",
    "400:\tlearn: 10.9506255\ttotal: 9.88s\tremaining: 2.44s\n",
    "499:\tlearn: 10.5096733\ttotal: 12.5s\tremaining: 0us\n",
    "\n",
    "Base 80/20 CatBoost Performance:\n",
    "\n",
    "RMSE: 10.8899\n",
    "\n",
    "R²: 0.8970\n",
    "\n",
    "Cross-validated RMSE: 11.4051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced features (top 99 selected by other model):\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Feature Engineering: Physics-Based Ratio, Thermal Conductivity Transformation, Log transformation\n",
    "merged_data[\"mass_density_ratio\"] = merged_data[\"wtd_mean_atomic_mass\"] / (merged_data[\"wtd_mean_Density\"] + 1e-9)\n",
    "merged_data[\"affinity_valence_ratio\"] = merged_data[\"wtd_mean_ElectronAffinity\"] / (merged_data[\"wtd_mean_Valence\"] + 1e-9)\n",
    "merged_data[\"log_thermal_conductivity\"] = np.log1p(merged_data[\"range_ThermalConductivity\"])\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"\n",
    "features = ['mean_atomic_mass', 'wtd_mean_atomic_mass', 'gmean_atomic_mass',\n",
    "       'entropy_atomic_mass', 'wtd_entropy_atomic_mass', 'range_atomic_mass',\n",
    "       'wtd_range_atomic_mass', 'wtd_std_atomic_mass', 'mean_fie',\n",
    "       'wtd_mean_fie', 'wtd_entropy_fie', 'range_fie', 'wtd_range_fie',\n",
    "       'wtd_std_fie', 'mean_atomic_radius', 'wtd_mean_atomic_radius',\n",
    "       'gmean_atomic_radius', 'range_atomic_radius', 'wtd_range_atomic_radius',\n",
    "       'mean_Density', 'wtd_mean_Density', 'gmean_Density', 'entropy_Density',\n",
    "       'wtd_entropy_Density', 'range_Density', 'wtd_range_Density',\n",
    "       'wtd_std_Density', 'mean_ElectronAffinity', 'wtd_mean_ElectronAffinity',\n",
    "       'gmean_ElectronAffinity', 'wtd_gmean_ElectronAffinity',\n",
    "       'entropy_ElectronAffinity', 'wtd_entropy_ElectronAffinity',\n",
    "       'range_ElectronAffinity', 'wtd_range_ElectronAffinity',\n",
    "       'wtd_std_ElectronAffinity', 'mean_FusionHeat', 'wtd_mean_FusionHeat',\n",
    "       'gmean_FusionHeat', 'entropy_FusionHeat', 'wtd_entropy_FusionHeat',\n",
    "       'range_FusionHeat', 'wtd_range_FusionHeat', 'wtd_std_FusionHeat',\n",
    "       'mean_ThermalConductivity', 'wtd_mean_ThermalConductivity',\n",
    "       'gmean_ThermalConductivity', 'wtd_gmean_ThermalConductivity',\n",
    "       'entropy_ThermalConductivity', 'wtd_entropy_ThermalConductivity',\n",
    "       'range_ThermalConductivity', 'wtd_range_ThermalConductivity',\n",
    "       'mean_Valence', 'wtd_mean_Valence', 'range_Valence',\n",
    "       'wtd_range_Valence', 'wtd_std_Valence', 'H', 'B', 'C', 'O', 'F', 'Na',\n",
    "       'Mg', 'Al', 'Cl', 'K', 'Ca', 'V', 'Cr', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
    "       'As', 'Se', 'Sr', 'Y', 'Nb', 'Sn', 'I', 'Ba', 'La', 'Ce', 'Pr', 'Nd',\n",
    "       'Sm', 'Eu', 'Gd', 'Tb', 'Yb', 'Hg', 'Tl', 'Pb', 'Bi',\n",
    "       'mass_density_ratio', 'affinity_valence_ratio',\n",
    "       'log_thermal_conductivity']\n",
    "X = merged_data[features]\n",
    "y = merged_data[target]\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a baseline CatBoost model\n",
    "\n",
    "\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=500,             # Number of boosting rounds (trees)\n",
    "    learning_rate=0.03,         # Step size for updating weights; lower values usually require more iterations but can lead to better generalization\n",
    "    depth=6,                    # Maximum depth of the trees; controls the model’s complexity\n",
    "    l2_leaf_reg=3,              # L2 regularization coefficient to reduce overfitting\n",
    "    loss_function='RMSE',       # Loss function to optimize, here RMSE for regression tasks\n",
    "    random_seed=42,             # Ensures reproducibility\n",
    "    verbose=100                 # Controls how often training progress is printed (set to 0 to disable)\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using RMSE and R² metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Base 80/20 CatBoost Performance:\")\n",
    "print(\"RMSE: {:.4f}\".format(rmse))\n",
    "print(\"R²: {:.4f}\".format(r2))\n",
    "\n",
    "# Optional Cross-validation to further assess model performance\n",
    "cv_scores = cross_val_score(catboost_model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "cv_rmse = -np.mean(cv_scores)\n",
    "print(\"Cross-validated RMSE: {:.4f}\".format(cv_rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results reduced (99 features) feature set:\n",
    "\n",
    "400:\tlearn: 10.9456783\ttotal: 9.22s\tremaining: 2.28s\n",
    "499:\tlearn: 10.5033401\ttotal: 11.4s\tremaining: 0us\n",
    "\n",
    "Base 80/20 CatBoost Performance:\n",
    "\n",
    "RMSE: 10.8467\n",
    "\n",
    "R²: 0.8978\n",
    "\n",
    "Cross-validated RMSE: 11.3898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization of the CatBoost using the 99 feature set\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Feature Engineering: Physics-Based Ratio, Thermal Conductivity Transformation, Log transformation\n",
    "merged_data[\"mass_density_ratio\"] = merged_data[\"wtd_mean_atomic_mass\"] / (merged_data[\"wtd_mean_Density\"] + 1e-9)\n",
    "merged_data[\"affinity_valence_ratio\"] = merged_data[\"wtd_mean_ElectronAffinity\"] / (merged_data[\"wtd_mean_Valence\"] + 1e-9)\n",
    "merged_data[\"log_thermal_conductivity\"] = np.log1p(merged_data[\"range_ThermalConductivity\"])\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"\n",
    "features = ['mean_atomic_mass', 'wtd_mean_atomic_mass', 'gmean_atomic_mass',\n",
    "       'entropy_atomic_mass', 'wtd_entropy_atomic_mass', 'range_atomic_mass',\n",
    "       'wtd_range_atomic_mass', 'wtd_std_atomic_mass', 'mean_fie',\n",
    "       'wtd_mean_fie', 'wtd_entropy_fie', 'range_fie', 'wtd_range_fie',\n",
    "       'wtd_std_fie', 'mean_atomic_radius', 'wtd_mean_atomic_radius',\n",
    "       'gmean_atomic_radius', 'range_atomic_radius', 'wtd_range_atomic_radius',\n",
    "       'mean_Density', 'wtd_mean_Density', 'gmean_Density', 'entropy_Density',\n",
    "       'wtd_entropy_Density', 'range_Density', 'wtd_range_Density',\n",
    "       'wtd_std_Density', 'mean_ElectronAffinity', 'wtd_mean_ElectronAffinity',\n",
    "       'gmean_ElectronAffinity', 'wtd_gmean_ElectronAffinity',\n",
    "       'entropy_ElectronAffinity', 'wtd_entropy_ElectronAffinity',\n",
    "       'range_ElectronAffinity', 'wtd_range_ElectronAffinity',\n",
    "       'wtd_std_ElectronAffinity', 'mean_FusionHeat', 'wtd_mean_FusionHeat',\n",
    "       'gmean_FusionHeat', 'entropy_FusionHeat', 'wtd_entropy_FusionHeat',\n",
    "       'range_FusionHeat', 'wtd_range_FusionHeat', 'wtd_std_FusionHeat',\n",
    "       'mean_ThermalConductivity', 'wtd_mean_ThermalConductivity',\n",
    "       'gmean_ThermalConductivity', 'wtd_gmean_ThermalConductivity',\n",
    "       'entropy_ThermalConductivity', 'wtd_entropy_ThermalConductivity',\n",
    "       'range_ThermalConductivity', 'wtd_range_ThermalConductivity',\n",
    "       'mean_Valence', 'wtd_mean_Valence', 'range_Valence',\n",
    "       'wtd_range_Valence', 'wtd_std_Valence', 'H', 'B', 'C', 'O', 'F', 'Na',\n",
    "       'Mg', 'Al', 'Cl', 'K', 'Ca', 'V', 'Cr', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
    "       'As', 'Se', 'Sr', 'Y', 'Nb', 'Sn', 'I', 'Ba', 'La', 'Ce', 'Pr', 'Nd',\n",
    "       'Sm', 'Eu', 'Gd', 'Tb', 'Yb', 'Hg', 'Tl', 'Pb', 'Bi',\n",
    "       'mass_density_ratio', 'affinity_valence_ratio',\n",
    "       'log_thermal_conductivity']\n",
    "X = merged_data[features]\n",
    "y = merged_data[target]\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True)\n",
    "    depth = trial.suggest_int(\"depth\", 4, 10)\n",
    "    l2_leaf_reg = trial.suggest_float(\"l2_leaf_reg\", 1, 10)\n",
    "    iterations = trial.suggest_int(\"iterations\", 100, 1000)\n",
    "    \n",
    "    # Initialize CatBoostRegressor with the suggested hyperparameters\n",
    "    model = CatBoostRegressor(\n",
    "        learning_rate=learning_rate,\n",
    "        depth=depth,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        iterations=iterations,\n",
    "        loss_function=\"RMSE\",\n",
    "        random_seed=42,\n",
    "        verbose=0  # Suppress verbose output\n",
    "    )\n",
    "    \n",
    "    # Evaluate using 5-fold cross-validation with negative RMSE scoring\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n",
    "    rmse = -np.mean(cv_scores)\n",
    "    return rmse\n",
    "\n",
    "# Create an Optuna study object, specifying that we want to minimize the objective\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"catboost_opt\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best RMSE:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "[I 2025-03-03 17:01:37,127] Trial 29 finished with value: 9.73032933391566 and parameters: {'learning_rate': 0.08464381700212753, 'depth': 10, 'l2_leaf_reg': 7.837898925719832, 'iterations': 503}. \n",
    "\n",
    "Best is trial 12 with value: 9.37859530361177.\n",
    "Best hyperparameters found:\n",
    "{'learning_rate': 0.09620611468282192, 'depth': 9, 'l2_leaf_reg': 4.192572316971277, 'iterations': 998}\n",
    "Best RMSE: 9.37859530361177"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
