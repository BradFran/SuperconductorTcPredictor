{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: RMSE = 8.2668, R² = 0.9404\n",
      "Run 2: RMSE = 8.8676, R² = 0.9313\n",
      "Run 3: RMSE = 8.6958, R² = 0.9343\n",
      "Run 4: RMSE = 8.4911, R² = 0.9378\n",
      "Run 5: RMSE = 9.1397, R² = 0.9275\n",
      "Run 6: RMSE = 8.6022, R² = 0.9343\n",
      "Run 7: RMSE = 8.9883, R² = 0.9315\n",
      "Run 8: RMSE = 8.7249, R² = 0.9343\n",
      "Run 9: RMSE = 8.2244, R² = 0.9427\n",
      "Run 10: RMSE = 8.3890, R² = 0.9406\n",
      "Run 11: RMSE = 8.9937, R² = 0.9331\n",
      "Run 12: RMSE = 9.7432, R² = 0.9218\n",
      "Run 13: RMSE = 8.6977, R² = 0.9356\n",
      "Run 14: RMSE = 8.2152, R² = 0.9420\n",
      "Run 15: RMSE = 8.6335, R² = 0.9383\n",
      "Run 16: RMSE = 9.1838, R² = 0.9267\n",
      "Run 17: RMSE = 8.5961, R² = 0.9371\n",
      "Run 18: RMSE = 8.5562, R² = 0.9376\n",
      "Run 19: RMSE = 9.2510, R² = 0.9254\n",
      "Run 20: RMSE = 8.6889, R² = 0.9342\n",
      "Run 21: RMSE = 8.9863, R² = 0.9315\n",
      "Run 22: RMSE = 9.6585, R² = 0.9206\n",
      "Run 23: RMSE = 8.6251, R² = 0.9372\n",
      "Run 24: RMSE = 9.3573, R² = 0.9265\n",
      "Run 25: RMSE = 8.4191, R² = 0.9395\n",
      "\n",
      "Average RMSE over 25 runs: 8.7998\n",
      "Average R² over 25 runs: 0.9337\n"
     ]
    }
   ],
   "source": [
    "# attempt to test best blended model with 25 runs similarly to how the author originally did\n",
    "# use a 90/10 train/test split and evaluate performance of 25 runs, each with different random samples\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Feature Engineering: Physics-Based Ratio, Thermal Conductivity Transformation, Log transformation\n",
    "merged_data[\"mass_density_ratio\"] = merged_data[\"wtd_mean_atomic_mass\"] / (merged_data[\"wtd_mean_Density\"] + 1e-9)\n",
    "merged_data[\"affinity_valence_ratio\"] = merged_data[\"wtd_mean_ElectronAffinity\"] / (merged_data[\"wtd_mean_Valence\"] + 1e-9)\n",
    "merged_data[\"log_thermal_conductivity\"] = np.log1p(merged_data[\"range_ThermalConductivity\"])\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"\n",
    "features = ['mean_atomic_mass', 'wtd_mean_atomic_mass', 'gmean_atomic_mass',\n",
    "       'entropy_atomic_mass', 'wtd_entropy_atomic_mass', 'range_atomic_mass',\n",
    "       'wtd_range_atomic_mass', 'wtd_std_atomic_mass', 'mean_fie',\n",
    "       'wtd_mean_fie', 'wtd_entropy_fie', 'range_fie', 'wtd_range_fie',\n",
    "       'wtd_std_fie', 'mean_atomic_radius', 'wtd_mean_atomic_radius',\n",
    "       'gmean_atomic_radius', 'range_atomic_radius', 'wtd_range_atomic_radius',\n",
    "       'mean_Density', 'wtd_mean_Density', 'gmean_Density', 'entropy_Density',\n",
    "       'wtd_entropy_Density', 'range_Density', 'wtd_range_Density',\n",
    "       'wtd_std_Density', 'mean_ElectronAffinity', 'wtd_mean_ElectronAffinity',\n",
    "       'gmean_ElectronAffinity', 'wtd_gmean_ElectronAffinity',\n",
    "       'entropy_ElectronAffinity', 'wtd_entropy_ElectronAffinity',\n",
    "       'range_ElectronAffinity', 'wtd_range_ElectronAffinity',\n",
    "       'wtd_std_ElectronAffinity', 'mean_FusionHeat', 'wtd_mean_FusionHeat',\n",
    "       'gmean_FusionHeat', 'entropy_FusionHeat', 'wtd_entropy_FusionHeat',\n",
    "       'range_FusionHeat', 'wtd_range_FusionHeat', 'wtd_std_FusionHeat',\n",
    "       'mean_ThermalConductivity', 'wtd_mean_ThermalConductivity',\n",
    "       'gmean_ThermalConductivity', 'wtd_gmean_ThermalConductivity',\n",
    "       'entropy_ThermalConductivity', 'wtd_entropy_ThermalConductivity',\n",
    "       'range_ThermalConductivity', 'wtd_range_ThermalConductivity',\n",
    "       'mean_Valence', 'wtd_mean_Valence', 'range_Valence',\n",
    "       'wtd_range_Valence', 'wtd_std_Valence', 'H', 'B', 'C', 'O', 'F', 'Na',\n",
    "       'Mg', 'Al', 'Cl', 'K', 'Ca', 'V', 'Cr', 'Fe', 'Co', 'Ni', 'Cu', 'Zn',\n",
    "       'As', 'Se', 'Sr', 'Y', 'Nb', 'Sn', 'I', 'Ba', 'La', 'Ce', 'Pr', 'Nd',\n",
    "       'Sm', 'Eu', 'Gd', 'Tb', 'Yb', 'Hg', 'Tl', 'Pb', 'Bi',\n",
    "       'mass_density_ratio', 'affinity_valence_ratio',\n",
    "       'log_thermal_conductivity']\n",
    "X = merged_data[features]\n",
    "y = merged_data[target]\n",
    "\n",
    "\n",
    "# Optimized LightGBM Model\n",
    "optimized_lgb = lgb.LGBMRegressor(n_estimators=496, max_depth=15, learning_rate=0.057878589503943714, \n",
    "                                  subsample=0.6619352139576826, colsample_bytree=0.7512301369524537, \n",
    "                                  num_leaves=148, force_col_wise=True, verbose=-1, random_state=42)\n",
    "\n",
    "\n",
    "# Optimized XGBoost Model\n",
    "optimized_xgb = xgb.XGBRegressor(n_estimators=407, max_depth=10, learning_rate=0.02962746174406205,\n",
    "                                 subsample=0.8786056663685927, colsample_bytree=0.6260167856358314,\n",
    "                                 gamma=4.321388407974591, tree_method=\"hist\", random_state=42)\n",
    "\n",
    "\n",
    "# Define blending weights\n",
    "best_weight_lgb = 0.3454  # Previously found optimal weight\n",
    "best_weight_xgb = 1.0 - best_weight_lgb\n",
    "\n",
    "\n",
    "# prepare for doing 25 runs as in the original paper\n",
    "n_runs = 25\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # Perform a 90/10 random split; vary the random_state for each iteration\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42 + i)\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    optimized_lgb.fit(X_train, y_train)\n",
    "    optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_lgb_test = optimized_lgb.predict(X_test)\n",
    "    y_pred_xgb_test = optimized_xgb.predict(X_test)\n",
    "    y_pred = (best_weight_lgb * y_pred_lgb_test) + (best_weight_xgb * y_pred_xgb_test)\n",
    "    \n",
    "    # Compute RMSE and R² for this run\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "    \n",
    "    print(f\"Run {i+1}: RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "# Compute the average RMSE and R² over the 25 runs\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_r2 = np.mean(r2_list)\n",
    "print(f\"\\nAverage RMSE over 25 runs: {avg_rmse:.4f}\")\n",
    "print(f\"Average R² over 25 runs: {avg_r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with 90/10 split:\n",
    "\n",
    "Run 1: RMSE = 8.2668, R² = 0.9404\n",
    "Run 2: RMSE = 8.8676, R² = 0.9313\n",
    "Run 3: RMSE = 8.6958, R² = 0.9343\n",
    "Run 4: RMSE = 8.4911, R² = 0.9378\n",
    "Run 5: RMSE = 9.1397, R² = 0.9275\n",
    "Run 6: RMSE = 8.6022, R² = 0.9343\n",
    "Run 7: RMSE = 8.9883, R² = 0.9315\n",
    "Run 8: RMSE = 8.7249, R² = 0.9343\n",
    "Run 9: RMSE = 8.2244, R² = 0.9427\n",
    "Run 10: RMSE = 8.3890, R² = 0.9406\n",
    "Run 11: RMSE = 8.9937, R² = 0.9331\n",
    "Run 12: RMSE = 9.7432, R² = 0.9218\n",
    "Run 13: RMSE = 8.6977, R² = 0.9356\n",
    "Run 14: RMSE = 8.2152, R² = 0.9420\n",
    "Run 15: RMSE = 8.6335, R² = 0.9383\n",
    "Run 16: RMSE = 9.1838, R² = 0.9267\n",
    "Run 17: RMSE = 8.5961, R² = 0.9371\n",
    "Run 18: RMSE = 8.5562, R² = 0.9376\n",
    "Run 19: RMSE = 9.2510, R² = 0.9254\n",
    "Run 20: RMSE = 8.6889, R² = 0.9342\n",
    "Run 21: RMSE = 8.9863, R² = 0.9315\n",
    "Run 22: RMSE = 9.6585, R² = 0.9206\n",
    "Run 23: RMSE = 8.6251, R² = 0.9372\n",
    "Run 24: RMSE = 9.3573, R² = 0.9265\n",
    "Run 25: RMSE = 8.4191, R² = 0.9395\n",
    "\n",
    "Average RMSE over 25 runs: 8.7998\n",
    "\n",
    "Average R² over 25 runs: 0.9337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: RMSE = 9.1013, R² = 0.9288\n",
      "Run 2: RMSE = 9.1598, R² = 0.9283\n",
      "Run 3: RMSE = 8.9797, R² = 0.9312\n",
      "Run 4: RMSE = 9.1756, R² = 0.9278\n",
      "Run 5: RMSE = 9.3386, R² = 0.9252\n",
      "Run 6: RMSE = 9.2190, R² = 0.9282\n",
      "Run 7: RMSE = 9.3420, R² = 0.9253\n",
      "Run 8: RMSE = 9.1238, R² = 0.9280\n",
      "Run 9: RMSE = 9.0236, R² = 0.9306\n",
      "Run 10: RMSE = 9.3054, R² = 0.9268\n",
      "Run 11: RMSE = 9.0739, R² = 0.9298\n",
      "Run 12: RMSE = 9.2928, R² = 0.9281\n",
      "Run 13: RMSE = 9.1072, R² = 0.9284\n",
      "Run 14: RMSE = 8.8354, R² = 0.9336\n",
      "Run 15: RMSE = 8.9876, R² = 0.9313\n",
      "Run 16: RMSE = 9.0327, R² = 0.9307\n",
      "Run 17: RMSE = 9.0940, R² = 0.9295\n",
      "Run 18: RMSE = 9.2818, R² = 0.9269\n",
      "Run 19: RMSE = 9.0650, R² = 0.9303\n",
      "Run 20: RMSE = 9.0698, R² = 0.9301\n",
      "Run 21: RMSE = 9.3236, R² = 0.9267\n",
      "Run 22: RMSE = 9.3789, R² = 0.9249\n",
      "Run 23: RMSE = 9.1169, R² = 0.9284\n",
      "Run 24: RMSE = 9.3344, R² = 0.9258\n",
      "Run 25: RMSE = 9.2138, R² = 0.9274\n",
      "\n",
      "Average RMSE over 25 runs: 9.1591\n",
      "Average R² over 25 runs: 0.9285\n"
     ]
    }
   ],
   "source": [
    "# now do it with 66/33 split\n",
    "\n",
    "# prepare for doing 25 runs as in the original paper\n",
    "n_runs = 25\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # Perform a 66/33 random split; vary the random_state for each iteration\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42 + i)\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    optimized_lgb.fit(X_train, y_train)\n",
    "    optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_lgb_test = optimized_lgb.predict(X_test)\n",
    "    y_pred_xgb_test = optimized_xgb.predict(X_test)\n",
    "    y_pred = (best_weight_lgb * y_pred_lgb_test) + (best_weight_xgb * y_pred_xgb_test)\n",
    "    \n",
    "    # Compute RMSE and R² for this run\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "    \n",
    "    print(f\"Run {i+1}: RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "# Compute the average RMSE and R² over the 25 runs\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_r2 = np.mean(r2_list)\n",
    "print(f\"\\nAverage RMSE over 25 runs: {avg_rmse:.4f}\")\n",
    "print(f\"Average R² over 25 runs: {avg_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results 33/66 split 25 runs:\n",
    "\n",
    "Run 1: RMSE = 9.1013, R² = 0.9288\n",
    "Run 2: RMSE = 9.1598, R² = 0.9283\n",
    "Run 3: RMSE = 8.9797, R² = 0.9312\n",
    "Run 4: RMSE = 9.1756, R² = 0.9278\n",
    "Run 5: RMSE = 9.3386, R² = 0.9252\n",
    "Run 6: RMSE = 9.2190, R² = 0.9282\n",
    "Run 7: RMSE = 9.3420, R² = 0.9253\n",
    "Run 8: RMSE = 9.1238, R² = 0.9280\n",
    "Run 9: RMSE = 9.0236, R² = 0.9306\n",
    "Run 10: RMSE = 9.3054, R² = 0.9268\n",
    "Run 11: RMSE = 9.0739, R² = 0.9298\n",
    "Run 12: RMSE = 9.2928, R² = 0.9281\n",
    "Run 13: RMSE = 9.1072, R² = 0.9284\n",
    "Run 14: RMSE = 8.8354, R² = 0.9336\n",
    "Run 15: RMSE = 8.9876, R² = 0.9313\n",
    "Run 16: RMSE = 9.0327, R² = 0.9307\n",
    "Run 17: RMSE = 9.0940, R² = 0.9295\n",
    "Run 18: RMSE = 9.2818, R² = 0.9269\n",
    "Run 19: RMSE = 9.0650, R² = 0.9303\n",
    "Run 20: RMSE = 9.0698, R² = 0.9301\n",
    "Run 21: RMSE = 9.3236, R² = 0.9267\n",
    "Run 22: RMSE = 9.3789, R² = 0.9249\n",
    "Run 23: RMSE = 9.1169, R² = 0.9284\n",
    "Run 24: RMSE = 9.3344, R² = 0.9258\n",
    "Run 25: RMSE = 9.2138, R² = 0.9274\n",
    "\n",
    "Average RMSE over 25 runs: 9.1591\n",
    "\n",
    "Average R² over 25 runs: 0.9285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: RMSE = 8.3812, R² = 0.9387\n",
      "Run 2: RMSE = 9.0486, R² = 0.9285\n",
      "Run 3: RMSE = 8.8819, R² = 0.9314\n",
      "Run 4: RMSE = 8.6493, R² = 0.9355\n",
      "Run 5: RMSE = 9.3036, R² = 0.9249\n",
      "Run 6: RMSE = 8.8660, R² = 0.9303\n",
      "Run 7: RMSE = 8.9372, R² = 0.9323\n",
      "Run 8: RMSE = 8.8508, R² = 0.9324\n",
      "Run 9: RMSE = 8.3508, R² = 0.9409\n",
      "Run 10: RMSE = 8.3315, R² = 0.9414\n",
      "Run 11: RMSE = 9.3023, R² = 0.9284\n",
      "Run 12: RMSE = 10.0210, R² = 0.9173\n",
      "Run 13: RMSE = 8.8528, R² = 0.9333\n",
      "Run 14: RMSE = 8.3912, R² = 0.9395\n",
      "Run 15: RMSE = 8.6283, R² = 0.9383\n",
      "Run 16: RMSE = 9.4258, R² = 0.9228\n",
      "Run 17: RMSE = 8.6154, R² = 0.9369\n",
      "Run 18: RMSE = 8.6523, R² = 0.9362\n",
      "Run 19: RMSE = 9.4458, R² = 0.9222\n",
      "Run 20: RMSE = 8.6627, R² = 0.9346\n",
      "Run 21: RMSE = 9.1779, R² = 0.9285\n",
      "Run 22: RMSE = 9.7199, R² = 0.9196\n",
      "Run 23: RMSE = 8.7445, R² = 0.9354\n",
      "Run 24: RMSE = 9.4600, R² = 0.9249\n",
      "Run 25: RMSE = 8.5770, R² = 0.9372\n",
      "\n",
      "Average RMSE over 25 runs: 8.9311\n",
      "Average R² over 25 runs: 0.9317\n"
     ]
    }
   ],
   "source": [
    "# now train and test the new model ONLY on the original features\n",
    "# use a 90/10 train/test split and evaluate performance of 25 runs, each with different random samples\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the data\n",
    "main_data = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# 'critical_temp' is the target\n",
    "X = main_data.drop('critical_temp', axis=1)\n",
    "y = main_data['critical_temp']\n",
    "\n",
    "\n",
    "# Optimized LightGBM Model\n",
    "optimized_lgb = lgb.LGBMRegressor(n_estimators=496, max_depth=15, learning_rate=0.057878589503943714, \n",
    "                                  subsample=0.6619352139576826, colsample_bytree=0.7512301369524537, \n",
    "                                  num_leaves=148, force_col_wise=True, verbose=-1, random_state=42)\n",
    "\n",
    "\n",
    "# Optimized XGBoost Model\n",
    "optimized_xgb = xgb.XGBRegressor(n_estimators=407, max_depth=10, learning_rate=0.02962746174406205,\n",
    "                                 subsample=0.8786056663685927, colsample_bytree=0.6260167856358314,\n",
    "                                 gamma=4.321388407974591, tree_method=\"hist\", random_state=42)\n",
    "\n",
    "\n",
    "# Define blending weights\n",
    "best_weight_lgb = 0.3454  # Previously found optimal weight\n",
    "best_weight_xgb = 1.0 - best_weight_lgb\n",
    "\n",
    "\n",
    "# prepare for doing 25 runs as in the original paper\n",
    "n_runs = 25\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # Perform a 90/10 random split; vary the random_state for each iteration\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42 + i)\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    optimized_lgb.fit(X_train, y_train)\n",
    "    optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_lgb_test = optimized_lgb.predict(X_test)\n",
    "    y_pred_xgb_test = optimized_xgb.predict(X_test)\n",
    "    y_pred = (best_weight_lgb * y_pred_lgb_test) + (best_weight_xgb * y_pred_xgb_test)\n",
    "    \n",
    "    # Compute RMSE and R² for this run\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "    \n",
    "    print(f\"Run {i+1}: RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "# Compute the average RMSE and R² over the 25 runs\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_r2 = np.mean(r2_list)\n",
    "print(f\"\\nAverage RMSE over 25 runs: {avg_rmse:.4f}\")\n",
    "print(f\"Average R² over 25 runs: {avg_r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 1: RMSE = 8.3812, R² = 0.9387\n",
    "Run 2: RMSE = 9.0486, R² = 0.9285\n",
    "Run 3: RMSE = 8.8819, R² = 0.9314\n",
    "Run 4: RMSE = 8.6493, R² = 0.9355\n",
    "Run 5: RMSE = 9.3036, R² = 0.9249\n",
    "Run 6: RMSE = 8.8660, R² = 0.9303\n",
    "Run 7: RMSE = 8.9372, R² = 0.9323\n",
    "Run 8: RMSE = 8.8508, R² = 0.9324\n",
    "Run 9: RMSE = 8.3508, R² = 0.9409\n",
    "Run 10: RMSE = 8.3315, R² = 0.9414\n",
    "Run 11: RMSE = 9.3023, R² = 0.9284\n",
    "Run 12: RMSE = 10.0210, R² = 0.9173\n",
    "Run 13: RMSE = 8.8528, R² = 0.9333\n",
    "Run 14: RMSE = 8.3912, R² = 0.9395\n",
    "Run 15: RMSE = 8.6283, R² = 0.9383\n",
    "Run 16: RMSE = 9.4258, R² = 0.9228\n",
    "Run 17: RMSE = 8.6154, R² = 0.9369\n",
    "Run 18: RMSE = 8.6523, R² = 0.9362\n",
    "Run 19: RMSE = 9.4458, R² = 0.9222\n",
    "Run 20: RMSE = 8.6627, R² = 0.9346\n",
    "Run 21: RMSE = 9.1779, R² = 0.9285\n",
    "Run 22: RMSE = 9.7199, R² = 0.9196\n",
    "Run 23: RMSE = 8.7445, R² = 0.9354\n",
    "Run 24: RMSE = 9.4600, R² = 0.9249\n",
    "Run 25: RMSE = 8.5770, R² = 0.9372\n",
    "\n",
    "Average RMSE over 25 runs: 8.9311\n",
    "Average R² over 25 runs: 0.9317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: RMSE = 9.2869, R² = 0.9258\n",
      "Run 2: RMSE = 9.3447, R² = 0.9254\n",
      "Run 3: RMSE = 9.2280, R² = 0.9274\n",
      "Run 4: RMSE = 9.2769, R² = 0.9262\n",
      "Run 5: RMSE = 9.3838, R² = 0.9245\n",
      "Run 6: RMSE = 9.4203, R² = 0.9251\n",
      "Run 7: RMSE = 9.4368, R² = 0.9238\n",
      "Run 8: RMSE = 9.2680, R² = 0.9257\n",
      "Run 9: RMSE = 9.1635, R² = 0.9284\n",
      "Run 10: RMSE = 9.4050, R² = 0.9252\n",
      "Run 11: RMSE = 9.3046, R² = 0.9262\n",
      "Run 12: RMSE = 9.5727, R² = 0.9237\n",
      "Run 13: RMSE = 9.2660, R² = 0.9259\n",
      "Run 14: RMSE = 9.0604, R² = 0.9302\n",
      "Run 15: RMSE = 9.1400, R² = 0.9289\n",
      "Run 16: RMSE = 9.3216, R² = 0.9262\n",
      "Run 17: RMSE = 9.1512, R² = 0.9286\n",
      "Run 18: RMSE = 9.4491, R² = 0.9243\n",
      "Run 19: RMSE = 9.2136, R² = 0.9279\n",
      "Run 20: RMSE = 9.1873, R² = 0.9283\n",
      "Run 21: RMSE = 9.4156, R² = 0.9252\n",
      "Run 22: RMSE = 9.4808, R² = 0.9232\n",
      "Run 23: RMSE = 9.3287, R² = 0.9250\n",
      "Run 24: RMSE = 9.4746, R² = 0.9235\n",
      "Run 25: RMSE = 9.3616, R² = 0.9250\n",
      "\n",
      "Average RMSE over 25 runs: 9.3177\n",
      "Average R² over 25 runs: 0.9260\n"
     ]
    }
   ],
   "source": [
    "# now train and test the new model ONLY on the original features\n",
    "# use the original 66/33 train/test split and evaluate performance of 25 runs, each with different random samples\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the data\n",
    "main_data = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# 'critical_temp' is the target\n",
    "X = main_data.drop('critical_temp', axis=1)\n",
    "y = main_data['critical_temp']\n",
    "\n",
    "\n",
    "# Optimized LightGBM Model\n",
    "optimized_lgb = lgb.LGBMRegressor(n_estimators=496, max_depth=15, learning_rate=0.057878589503943714, \n",
    "                                  subsample=0.6619352139576826, colsample_bytree=0.7512301369524537, \n",
    "                                  num_leaves=148, force_col_wise=True, verbose=-1, random_state=42)\n",
    "\n",
    "\n",
    "# Optimized XGBoost Model\n",
    "optimized_xgb = xgb.XGBRegressor(n_estimators=407, max_depth=10, learning_rate=0.02962746174406205,\n",
    "                                 subsample=0.8786056663685927, colsample_bytree=0.6260167856358314,\n",
    "                                 gamma=4.321388407974591, tree_method=\"hist\", random_state=42)\n",
    "\n",
    "\n",
    "# Define blending weights\n",
    "best_weight_lgb = 0.3454  # Previously found optimal weight\n",
    "best_weight_xgb = 1.0 - best_weight_lgb\n",
    "\n",
    "\n",
    "# prepare for doing 25 runs as in the original paper\n",
    "n_runs = 25\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # Perform a 66/33 random split; vary the random_state for each iteration\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42 + i)\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    optimized_lgb.fit(X_train, y_train)\n",
    "    optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_lgb_test = optimized_lgb.predict(X_test)\n",
    "    y_pred_xgb_test = optimized_xgb.predict(X_test)\n",
    "    y_pred = (best_weight_lgb * y_pred_lgb_test) + (best_weight_xgb * y_pred_xgb_test)\n",
    "    \n",
    "    # Compute RMSE and R² for this run\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "    \n",
    "    print(f\"Run {i+1}: RMSE = {rmse:.4f}, R² = {r2:.4f}\")\n",
    "\n",
    "# Compute the average RMSE and R² over the 25 runs\n",
    "avg_rmse = np.mean(rmse_list)\n",
    "avg_r2 = np.mean(r2_list)\n",
    "print(f\"\\nAverage RMSE over 25 runs: {avg_rmse:.4f}\")\n",
    "print(f\"Average R² over 25 runs: {avg_r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results original features with 66/33 split on new model:\n",
    "\n",
    "Run 1: RMSE = 9.2869, R² = 0.9258\n",
    "Run 2: RMSE = 9.3447, R² = 0.9254\n",
    "Run 3: RMSE = 9.2280, R² = 0.9274\n",
    "Run 4: RMSE = 9.2769, R² = 0.9262\n",
    "Run 5: RMSE = 9.3838, R² = 0.9245\n",
    "Run 6: RMSE = 9.4203, R² = 0.9251\n",
    "Run 7: RMSE = 9.4368, R² = 0.9238\n",
    "Run 8: RMSE = 9.2680, R² = 0.9257\n",
    "Run 9: RMSE = 9.1635, R² = 0.9284\n",
    "Run 10: RMSE = 9.4050, R² = 0.9252\n",
    "Run 11: RMSE = 9.3046, R² = 0.9262\n",
    "Run 12: RMSE = 9.5727, R² = 0.9237\n",
    "Run 13: RMSE = 9.2660, R² = 0.9259\n",
    "Run 14: RMSE = 9.0604, R² = 0.9302\n",
    "Run 15: RMSE = 9.1400, R² = 0.9289\n",
    "Run 16: RMSE = 9.3216, R² = 0.9262\n",
    "Run 17: RMSE = 9.1512, R² = 0.9286\n",
    "Run 18: RMSE = 9.4491, R² = 0.9243\n",
    "Run 19: RMSE = 9.2136, R² = 0.9279\n",
    "Run 20: RMSE = 9.1873, R² = 0.9283\n",
    "Run 21: RMSE = 9.4156, R² = 0.9252\n",
    "Run 22: RMSE = 9.4808, R² = 0.9232\n",
    "Run 23: RMSE = 9.3287, R² = 0.9250\n",
    "Run 24: RMSE = 9.4746, R² = 0.9235\n",
    "Run 25: RMSE = 9.3616, R² = 0.9250\n",
    "\n",
    "Average RMSE over 25 runs: 9.3177\n",
    "\n",
    "Average R² over 25 runs: 0.9260"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
