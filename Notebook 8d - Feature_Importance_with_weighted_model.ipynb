{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to test feature importance with the weighted XGBoost and LightGBM model ensemble\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"  # Target variable\n",
    "X = merged_data.drop(columns=[target, \"material\"])  # Drop 'material' column\n",
    "y = merged_data[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Optimized LightGBM Model\n",
    "optimized_lgb = lgb.LGBMRegressor(n_estimators=496, max_depth=15, learning_rate=0.057878589503943714, \n",
    "                                  subsample=0.6619352139576826, colsample_bytree=0.7512301369524537, \n",
    "                                  num_leaves=148, verbose=-1, force_col_wise=True)\n",
    "optimized_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Train Optimized XGBoost Model\n",
    "optimized_xgb = xgb.XGBRegressor(n_estimators=407, max_depth=10, learning_rate=0.02962746174406205,\n",
    "                                 subsample=0.8786056663685927, colsample_bytree=0.6260167856358314,\n",
    "                                 gamma=4.321388407974591, tree_method=\"hist\", random_state=42)\n",
    "optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_lgb_test = optimized_lgb.predict(X_test)\n",
    "y_pred_xgb_test = optimized_xgb.predict(X_test)\n",
    "\n",
    "# Define Bayesian Optimization for Blending Weights\n",
    "def objective(trial):\n",
    "    weight_lgb = trial.suggest_float(\"weight_lgb\", 0.0, 1.0)\n",
    "    weight_xgb = 1.0 - weight_lgb  # Ensure sum is 1\n",
    "    y_pred_ensemble = (weight_lgb * y_pred_lgb_test) + (weight_xgb * y_pred_xgb_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "\n",
    "# Run Bayesian Optimization for 50 trials\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best Weights from Optimization\n",
    "best_weight_lgb = study.best_params[\"weight_lgb\"]\n",
    "best_weight_xgb = 1.0 - best_weight_lgb\n",
    "\n",
    "# Apply Best Weights\n",
    "y_pred_ensemble_optimized = (best_weight_lgb * y_pred_lgb_test) + (best_weight_xgb * y_pred_xgb_test)\n",
    "\n",
    "# Evaluate Optimized Blended Model\n",
    "ensemble_rmse_opt = np.sqrt(mean_squared_error(y_test, y_pred_ensemble_optimized))\n",
    "ensemble_r2_opt = r2_score(y_test, y_pred_ensemble_optimized)\n",
    "\n",
    "print(f\"Optimized Weighted Blended Model - Test RMSE: {ensemble_rmse_opt:.4f}, Test R²: {ensemble_r2_opt:.4f}\")\n",
    "print(f\"Optimal Blending Weights: LightGBM={best_weight_lgb:.4f}, XGBoost={best_weight_xgb:.4f}\")\n",
    "\n",
    "# Feature Importance Analysis\n",
    "lgb_importance = pd.Series(optimized_lgb.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "xgb_importance = pd.Series(optimized_xgb.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "\n",
    "# Normalize importance scores\n",
    "lgb_importance = lgb_importance / lgb_importance.sum()\n",
    "xgb_importance = xgb_importance / xgb_importance.sum()\n",
    "\n",
    "# Compute blended feature importance\n",
    "ensemble_importance = (best_weight_lgb * lgb_importance) + (best_weight_xgb * xgb_importance)\n",
    "ensemble_importance = ensemble_importance.sort_values(ascending=False)\n",
    "\n",
    "# Plot Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "ensemble_importance[:20].plot(kind='barh')  # Show top 20 features\n",
    "plt.xlabel(\"Blended Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top Feature Importances in Optimized Blended Model\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"Top 20 Features in Optimized Blended Model:\")\n",
    "print(ensemble_importance[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "[I 2025-02-25 18:23:37,719] Trial 49 finished with value: 8.514911745102962 and parameters: {'weight_lgb': 0.1303015207206778}. Best is trial 22 with value: 8.502434908752939.\n",
    "Optimized Weighted Blended Model - Test RMSE: 8.5024, Test R²: 0.9372\n",
    "Optimal Blending Weights: LightGBM=0.3507, XGBoost=0.6493\n",
    "\n",
    "\n",
    "Top 20 Features in Optimized Blended Model:\n",
    "range_ThermalConductivity          0.282304\n",
    "Cu                                 0.175546\n",
    "range_atomic_radius                0.029883\n",
    "Ca                                 0.017167\n",
    "Ba                                 0.012077\n",
    "wtd_std_ElectronAffinity           0.010526\n",
    "wtd_gmean_Density                  0.008571\n",
    "wtd_gmean_ThermalConductivity      0.008373\n",
    "wtd_std_Valence                    0.008336\n",
    "wtd_entropy_ThermalConductivity    0.008035\n",
    "wtd_entropy_ElectronAffinity       0.007918\n",
    "wtd_gmean_Valence                  0.007916\n",
    "wtd_range_Valence                  0.007643\n",
    "wtd_std_ThermalConductivity        0.007577\n",
    "wtd_range_atomic_radius            0.007523\n",
    "gmean_Valence                      0.007404\n",
    "wtd_entropy_atomic_mass            0.007300\n",
    "wtd_entropy_FusionHeat             0.007222\n",
    "wtd_std_atomic_radius              0.007204\n",
    "wtd_entropy_Density                0.007102"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
