{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this compares the twp models I want to evaluate: RF and XGBoost \n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "main_data = pd.read_csv(\"./data/train.csv\")  # Superconductivity dataset\n",
    "unique_m = pd.read_csv(\"./data/unique_m.csv\")\n",
    "\n",
    "# Remove 'critical_temp' from unique_m to avoid duplication\n",
    "unique_m = unique_m.drop(columns=[\"critical_temp\"], errors='ignore')\n",
    "\n",
    "# Merge datasets assuming rows align (index-based merge)\n",
    "merged_data = pd.concat([main_data, unique_m], axis=1)\n",
    "\n",
    "# Define target and features\n",
    "target = \"critical_temp\"  # Target variable\n",
    "X = merged_data.drop(columns=[target, \"material\"])  # Drop 'material' column\n",
    "y = merged_data[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost Model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=200, max_depth=6, tree_method=\"hist\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost Performance\n",
    "train_rmse_xgb = np.sqrt(mean_squared_error(y_train, xgb_model.predict(X_train)))\n",
    "test_rmse_xgb = np.sqrt(mean_squared_error(y_test, xgb_model.predict(X_test)))\n",
    "train_r2_xgb = r2_score(y_train, xgb_model.predict(X_train))\n",
    "test_r2_xgb = r2_score(y_test, xgb_model.predict(X_test))\n",
    "\n",
    "print(f\"XGBoost - Train RMSE: {train_rmse_xgb:.4f}, Train R²: {train_r2_xgb:.4f}\")\n",
    "print(f\"XGBoost - Test RMSE: {test_rmse_xgb:.4f}, Test R²: {test_r2_xgb:.4f}\")\n",
    "\n",
    "# Train Random Forest Model\n",
    "rf_model = RandomForestRegressor(n_estimators=200, max_depth=6, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest Performance\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, rf_model.predict(X_train)))\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, rf_model.predict(X_test)))\n",
    "train_r2_rf = r2_score(y_train, rf_model.predict(X_train))\n",
    "test_r2_rf = r2_score(y_test, rf_model.predict(X_test))\n",
    "\n",
    "print(f\"Random Forest - Train RMSE: {train_rmse_rf:.4f}, Train R²: {train_r2_rf:.4f}\")\n",
    "print(f\"Random Forest - Test RMSE: {test_rmse_rf:.4f}, Test R²: {test_r2_rf:.4f}\")\n",
    "\n",
    "# Feature Importance (XGBoost)\n",
    "xgb_importance = pd.Series(xgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "xgb_importance.to_csv(\"feature_importance_ranking_xgb.csv\")\n",
    "\n",
    "# Feature Importance (Random Forest)\n",
    "rf_importance = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "rf_importance.to_csv(\"feature_importance_ranking_rf.csv\")\n",
    "\n",
    "# Print ranked features for comparison\n",
    "print(\"Feature Importance Ranking (XGBoost):\")\n",
    "print(xgb_importance)\n",
    "print(\"\\nFeature Importance Ranking (Random Forest):\")\n",
    "print(rf_importance)\n",
    "\n",
    "# Plot Feature Importance Side-by-Side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "axes[0].barh(xgb_importance.index[:20], xgb_importance.values[:20])\n",
    "axes[0].set_title(\"XGBoost Feature Importance\")\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "axes[1].barh(rf_importance.index[:20], rf_importance.values[:20])\n",
    "axes[1].set_title(\"Random Forest Feature Importance\")\n",
    "axes[1].set_xlabel(\"Importance\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "XGBoost - Train RMSE: 5.2047, Train R²: 0.9770\n",
    "XGBoost - Test RMSE: 8.9142, Test R²: 0.9310\n",
    "Random Forest - Train RMSE: 13.0025, Train R²: 0.8566\n",
    "Random Forest - Test RMSE: 13.1421, Test R²: 0.8500\n",
    "\n",
    "Feature Importance Ranking (XGBoost):\n",
    "Cu                           0.631333\n",
    "Ba                           0.051469\n",
    "gmean_Valence                0.046137\n",
    "Ca                           0.029114\n",
    "range_ThermalConductivity    0.020274\n",
    "...   \n",
    "Hf                           0.000000\n",
    "Os                           0.000000\n",
    "Po                           0.000000\n",
    "At                           0.000000\n",
    "Rn                           0.000000\n",
    "Length: 167, dtype: float32\n",
    "\n",
    "Feature Importance Ranking (Random Forest):\n",
    "Cu                   0.720902\n",
    "Ca                   0.055018\n",
    "Ba                   0.031046\n",
    "gmean_Valence        0.021857\n",
    "wtd_gmean_Density    0.021306\n",
    "...  \n",
    "Ta                   0.000000\n",
    "Au                   0.000000\n",
    "Po                   0.000000\n",
    "At                   0.000000\n",
    "Rn                   0.000000\n",
    "\n",
    "The RF is clearly outperformed by the XGBoot on this dataset. They both have similar features importance, but not exactly the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
